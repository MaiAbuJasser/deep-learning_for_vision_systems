{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2968639b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80377b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_18768\\3998107238.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 81s 4s/step - loss: 3.3542 - accuracy: 0.2791 - val_loss: 1.6612 - val_accuracy: 0.4556\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 77s 4s/step - loss: 1.4610 - accuracy: 0.4593 - val_loss: 1.1111 - val_accuracy: 0.5556\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 71s 4s/step - loss: 0.7877 - accuracy: 0.6944 - val_loss: 0.5143 - val_accuracy: 0.8444\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.4280 - accuracy: 0.8556 - val_loss: 0.2317 - val_accuracy: 0.9667\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.1640 - accuracy: 0.9500 - val_loss: 0.2679 - val_accuracy: 0.9444\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 74s 4s/step - loss: 0.1983 - accuracy: 0.9333 - val_loss: 0.3023 - val_accuracy: 0.8889\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 73s 4s/step - loss: 0.1790 - accuracy: 0.9611 - val_loss: 0.1695 - val_accuracy: 0.9889\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.1288 - accuracy: 0.9556 - val_loss: 0.0845 - val_accuracy: 0.9778\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 77s 4s/step - loss: 0.3022 - accuracy: 0.9167 - val_loss: 0.2274 - val_accuracy: 0.8889\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 79s 4s/step - loss: 0.1430 - accuracy: 0.9500 - val_loss: 0.1651 - val_accuracy: 0.9333\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.0753 - accuracy: 0.9767 - val_loss: 0.0452 - val_accuracy: 0.9889\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.0377 - accuracy: 0.9884 - val_loss: 0.1334 - val_accuracy: 0.9667\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.0251 - accuracy: 0.9889 - val_loss: 0.1585 - val_accuracy: 0.9778\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 132s 8s/step - loss: 0.0715 - accuracy: 0.9722 - val_loss: 0.1224 - val_accuracy: 0.9889\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 118s 7s/step - loss: 0.0279 - accuracy: 0.9889 - val_loss: 0.0764 - val_accuracy: 0.9778\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 91s 5s/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.1288 - val_accuracy: 0.9556\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 74s 4s/step - loss: 0.0889 - accuracy: 0.9667 - val_loss: 0.0939 - val_accuracy: 0.9667\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 71s 4s/step - loss: 0.0709 - accuracy: 0.9778 - val_loss: 0.0721 - val_accuracy: 0.9667\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 73s 4s/step - loss: 0.0444 - accuracy: 0.9889 - val_loss: 0.0540 - val_accuracy: 0.9778\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.0206 - accuracy: 0.9944 - val_loss: 0.1734 - val_accuracy: 0.9556\n",
      "Training time: 1673.330 seconds\n",
      "Training time in minutes: 27.889 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eafe092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 487.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 485.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 69s 7s/step - loss: 0.2078 - accuracy: 0.9400\n",
      "\n",
      "val loss: 0.2078\n",
      "val accuracy: 0.9400\n",
      "2/2 [==============================] - 12s 4s/step - loss: 0.1917 - accuracy: 0.9200\n",
      "\n",
      "Testing loss: 0.1917\n",
      "Testing accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "  def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5144ebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2bc8f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying data augmentation to images in folder 0\n",
      "Applying data augmentation to images in folder 1\n",
      "Applying data augmentation to images in folder 2\n",
      "Applying data augmentation to images in folder 3\n",
      "Applying data augmentation to images in folder 4\n",
      "Applying data augmentation to images in folder 5\n",
      "Applying data augmentation to images in folder 6\n",
      "Applying data augmentation to images in folder 7\n",
      "Applying data augmentation to images in folder 8\n",
      "Applying data augmentation to images in folder 9\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "import os\n",
    "# Define the data augmentation functions\n",
    "def random_crop(image):\n",
    "    # Randomly crop a portion of the image\n",
    "    (h, w) = image.shape[:2]\n",
    "    crop_h, crop_w = int(h * 0.8), int(w * 0.8)\n",
    "    top = random.randint(0, h - crop_h)\n",
    "    left = random.randint(0, w - crop_w)\n",
    "    bottom = top + crop_h\n",
    "    right = left + crop_w\n",
    "    cropped = image[top:bottom, left:right]\n",
    "    return cropped\n",
    "\n",
    "def random_zoom(image):\n",
    "    # Randomly zoom in or out on the image\n",
    "    (h, w) = image.shape[:2]\n",
    "    zoom_scale = random.uniform(1.0, 1.5)\n",
    "    new_h, new_w = int(h * zoom_scale), int(w * zoom_scale)\n",
    "    top = random.randint(0, new_h - h)\n",
    "    left = random.randint(0, new_w - w)\n",
    "    bottom = top + h\n",
    "    right = left + w\n",
    "    zoomed = cv2.resize(image[top:bottom, left:right], (w, h))\n",
    "    return zoomed\n",
    "\n",
    "def adjust_brightness(image):\n",
    "    # Adjust the brightness of the image\n",
    "    alpha = random.uniform(0.5, 1.5)\n",
    "    beta = random.randint(-50, 50)\n",
    "    adjusted = cv2.addWeighted(image, alpha, image, 0, beta)\n",
    "    return adjusted\n",
    "\n",
    "def adjust_contrast(image):\n",
    "    # Adjust the contrast of the image\n",
    "    alpha = random.uniform(0.5, 1.5)\n",
    "    mean = cv2.mean(image)[0]\n",
    "    adjusted = cv2.addWeighted(image, alpha, image, 0, mean * (1 - alpha))\n",
    "    return adjusted\n",
    "\n",
    "def random_color_jitter(image):\n",
    "    # Randomly adjust the hue, saturation, and brightness of the image\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hue_shift = random.randint(0, 30)\n",
    "    hsv[:, :, 0] = hsv[:, :, 0] + hue_shift\n",
    "    saturation_scale = random.uniform(0.5, 1.5)\n",
    "    hsv[:, :, 1] = hsv[:, :, 1] * saturation_scale\n",
    "    brightness_scale = random.uniform(0.5, 1.5)\n",
    "    hsv[:, :, 2] = hsv[:, :, 2] * brightness_scale\n",
    "    jittered = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    return jittered\n",
    "\n",
    "def add_gaussian_noise(image):\n",
    "    # Add Gaussian noise to the image\n",
    "    mean = random.randint(0, 20)\n",
    "    std_dev = random.uniform(0.0, 10.0)\n",
    "    noise = image.copy()\n",
    "    cv2.randn(noise, mean, std_dev)\n",
    "    noisy = cv2.add(image, noise)\n",
    "    return noisy\n",
    "\n",
    "def apply_blur(image):\n",
    "    # Apply a blur filter to the image\n",
    "    k_size = random.choice([3, 5, 7])\n",
    "    blurred = cv2.GaussianBlur(image, (k_size, k_size), 0)\n",
    "    return blurred\n",
    "\n",
    "def rotate_image(image):\n",
    "    # Rotate the image by 45 degrees\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, 45, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h))\n",
    "    return rotated\n",
    "\n",
    "def flip_image(image):\n",
    "    # Flip the image horizontally\n",
    "    flipped = cv2.flip(image, 1)\n",
    "    return flipped\n",
    "\n",
    "    # Loop through each folder\n",
    "for i in range(10):\n",
    "    folder_name = str(i)\n",
    "    print(f\"Applying data augmentation to images in folder {folder_name}\")\n",
    "    \n",
    "    # Get the path to the folder\n",
    "    folder_path = os.path.join(\"C:/Users/subeh/OneDrive/Desktop/deep_learnin/chapter_06/sign_language_project/dataset/train\", folder_name)\n",
    "\n",
    "    # Loop through each image in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Load the image\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        cropped_img = random_crop(img)\n",
    "        zoomed_img = random_zoom(img)\n",
    "        adjusted_img = adjust_brightness(img)\n",
    "        jittered_img = random_color_jitter(img)\n",
    "        noisy_img = add_gaussian_noise(img)\n",
    "        blurred_img = apply_blur(img)\n",
    "        rotated_img = rotate_image(img)\n",
    "        flipped_img = flip_image(img)\n",
    "        cv2.imwrite(os.path.join(folder_path, f\"{filename.split('.')[0]}_cropped.jpg\"), cropped_img)\n",
    "        cv2.imwrite(os.path.join(folder_path, f\"{filename.split('.')[0]}_zoomed.jpg\"), zoomed_img)\n",
    "        cv2.imwrite(os.path.join(folder_path, f\"{filename.split('.')[0]}_adjusted.jpg\"), adjusted_img)\n",
    "        cv2.imwrite(os.path.join(folder_path, f\"{filename.split('.')[0]}_jittered.jpg\"), jittered_img)\n",
    "        cv2.imwrite(os.path.join(folder_path, f\"{filename.split('.')[0]}_noisy.jpg\"), noisy_img)\n",
    "        cv2.imwrite(os.path.join(folder_path, f\"{filename.split('.')[0]}_blurred.jpg\"), blurred_img)\n",
    "        cv2.imwrite(os.path.join(folder_path, f\"{filename.split('.')[0]}_rotated.jpg\"), rotated_img)\n",
    "        cv2.imwrite(os.path.join(folder_path, f\"{filename.split('.')[0]}_flipped.jpg\"), flipped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb0ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15408 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db88420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_23476\\3998107238.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 74s 4s/step - loss: 3.2871 - accuracy: 0.1333 - val_loss: 2.0747 - val_accuracy: 0.1556\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 73s 4s/step - loss: 2.0650 - accuracy: 0.1944 - val_loss: 1.6372 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 76s 4s/step - loss: 1.5648 - accuracy: 0.5000 - val_loss: 1.0870 - val_accuracy: 0.6444\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 73s 4s/step - loss: 1.1051 - accuracy: 0.6056 - val_loss: 0.7069 - val_accuracy: 0.7778\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.8900 - accuracy: 0.6722 - val_loss: 0.3095 - val_accuracy: 0.9000\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 0.7143 - accuracy: 0.7833 - val_loss: 0.8361 - val_accuracy: 0.6667\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 75s 4s/step - loss: 0.7026 - accuracy: 0.7611 - val_loss: 0.7527 - val_accuracy: 0.7444\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 71s 4s/step - loss: 0.4705 - accuracy: 0.8444 - val_loss: 0.4652 - val_accuracy: 0.8222\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 71s 4s/step - loss: 0.4671 - accuracy: 0.8056 - val_loss: 0.3749 - val_accuracy: 0.8889\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 0.5209 - accuracy: 0.8556 - val_loss: 0.1286 - val_accuracy: 0.9778\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 74s 4s/step - loss: 0.3806 - accuracy: 0.8611 - val_loss: 0.2583 - val_accuracy: 0.9333\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 0.2142 - accuracy: 0.9222 - val_loss: 0.1127 - val_accuracy: 0.9667\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 91s 5s/step - loss: 0.2012 - accuracy: 0.9167 - val_loss: 0.1547 - val_accuracy: 0.9222\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 0.3676 - accuracy: 0.9111 - val_loss: 0.3670 - val_accuracy: 0.9000\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 73s 4s/step - loss: 0.2923 - accuracy: 0.9111 - val_loss: 0.0868 - val_accuracy: 0.9778\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 0.1618 - accuracy: 0.9389 - val_loss: 0.0651 - val_accuracy: 0.9889\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 0.2642 - accuracy: 0.9056 - val_loss: 0.1946 - val_accuracy: 0.9444\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.3779 - accuracy: 0.8722 - val_loss: 0.1037 - val_accuracy: 0.9667\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 71s 4s/step - loss: 0.2132 - accuracy: 0.9389 - val_loss: 0.0699 - val_accuracy: 0.9889\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 75s 4s/step - loss: 0.1822 - accuracy: 0.9444 - val_loss: 0.2000 - val_accuracy: 0.9667\n",
      "Training time: 1478.096 seconds\n",
      "Training time in minutes: 24.635 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.  } minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917ab4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 520.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 510.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 69s 7s/step - loss: 0.3015 - accuracy: 0.9067\n",
      "\n",
      "val loss: 0.3015\n",
      "val accuracy: 0.9067\n",
      "2/2 [==============================] - 12s 4s/step - loss: 0.2620 - accuracy: 0.8800\n",
      "\n",
      "Testing loss: 0.2620\n",
      "Testing accuracy: 0.8800\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341f892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462a3aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15408 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "186b64c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_23476\\13107789.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 79s 4s/step - loss: 3.4729 - accuracy: 0.1278 - val_loss: 2.2685 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 2.2208 - accuracy: 0.1167 - val_loss: 2.1704 - val_accuracy: 0.2667\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 2.0416 - accuracy: 0.2556 - val_loss: 1.8336 - val_accuracy: 0.3778\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 1.8876 - accuracy: 0.3833 - val_loss: 1.4131 - val_accuracy: 0.6111\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 74s 4s/step - loss: 1.6845 - accuracy: 0.4222 - val_loss: 1.3071 - val_accuracy: 0.5333\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 1.3564 - accuracy: 0.5556 - val_loss: 1.0786 - val_accuracy: 0.6222\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 1.3054 - accuracy: 0.5944 - val_loss: 0.7459 - val_accuracy: 0.8111\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 71s 4s/step - loss: 0.8923 - accuracy: 0.7056 - val_loss: 0.5735 - val_accuracy: 0.8222\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 73s 4s/step - loss: 0.6790 - accuracy: 0.8000 - val_loss: 0.3167 - val_accuracy: 0.8889\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.6373 - accuracy: 0.7500 - val_loss: 0.4211 - val_accuracy: 0.8444\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 71s 4s/step - loss: 0.5770 - accuracy: 0.7944 - val_loss: 0.2011 - val_accuracy: 0.9444\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.4139 - accuracy: 0.8667 - val_loss: 0.1359 - val_accuracy: 0.9778\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 73s 4s/step - loss: 0.4717 - accuracy: 0.8444 - val_loss: 0.2334 - val_accuracy: 0.9333\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 73s 4s/step - loss: 0.3631 - accuracy: 0.9000 - val_loss: 0.2744 - val_accuracy: 0.9556\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.3064 - accuracy: 0.9056 - val_loss: 0.2858 - val_accuracy: 0.8889\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.2771 - accuracy: 0.8778 - val_loss: 0.1318 - val_accuracy: 0.9444\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.3868 - accuracy: 0.9056 - val_loss: 0.0896 - val_accuracy: 0.9778\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 75s 4s/step - loss: 0.3822 - accuracy: 0.8889 - val_loss: 0.3051 - val_accuracy: 0.9000\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 71s 4s/step - loss: 0.2240 - accuracy: 0.9111 - val_loss: 0.0381 - val_accuracy: 0.9778\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 0.3440 - accuracy: 0.9333 - val_loss: 0.1357 - val_accuracy: 0.9444\n",
      "Training time: 1519.075 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Format specifier missing precision",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m train_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining time in minutes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_time_in_minutes\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.  \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Format specifier missing precision"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_1')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba7009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 413.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 478.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 69s 7s/step - loss: 0.2292 - accuracy: 0.9500\n",
      "\n",
      "val loss: 0.2292\n",
      "val accuracy: 0.9500\n",
      "2/2 [==============================] - 12s 4s/step - loss: 0.1409 - accuracy: 0.9800\n",
      "\n",
      "Testing loss: 0.1409\n",
      "Testing accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d5c4018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c035cff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893263af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_17512\\455968399.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 136s 8s/step - loss: 3.2281 - accuracy: 0.1333 - val_loss: 2.2986 - val_accuracy: 0.1000\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 160s 9s/step - loss: 2.2961 - accuracy: 0.1500 - val_loss: 2.3013 - val_accuracy: 0.0667\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 128s 7s/step - loss: 2.3134 - accuracy: 0.0889 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 117s 7s/step - loss: 2.3038 - accuracy: 0.0944 - val_loss: 2.3022 - val_accuracy: 0.1111\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 119s 7s/step - loss: 2.3023 - accuracy: 0.1333 - val_loss: 2.3018 - val_accuracy: 0.1333\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 113s 6s/step - loss: 2.3019 - accuracy: 0.1611 - val_loss: 2.3017 - val_accuracy: 0.1333\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 114s 6s/step - loss: 2.3015 - accuracy: 0.1512 - val_loss: 2.3014 - val_accuracy: 0.1222\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 123s 7s/step - loss: 2.3020 - accuracy: 0.1000 - val_loss: 2.3002 - val_accuracy: 0.1222\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 119s 7s/step - loss: 2.3008 - accuracy: 0.1278 - val_loss: 2.3004 - val_accuracy: 0.1778\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 117s 7s/step - loss: 2.3008 - accuracy: 0.1611 - val_loss: 2.3039 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 102s 6s/step - loss: 2.3027 - accuracy: 0.1047 - val_loss: 2.3019 - val_accuracy: 0.1111\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 104s 6s/step - loss: 2.3010 - accuracy: 0.1278 - val_loss: 2.3025 - val_accuracy: 0.0667\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 104s 6s/step - loss: 2.3003 - accuracy: 0.1167 - val_loss: 2.2968 - val_accuracy: 0.1111\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 102s 6s/step - loss: 2.3003 - accuracy: 0.0988 - val_loss: 2.3310 - val_accuracy: 0.0667\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 101s 6s/step - loss: 2.2670 - accuracy: 0.1395 - val_loss: 2.2910 - val_accuracy: 0.1333\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 104s 6s/step - loss: 2.3085 - accuracy: 0.1167 - val_loss: 2.3601 - val_accuracy: 0.0889\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 105s 6s/step - loss: 2.3044 - accuracy: 0.1556 - val_loss: 2.2980 - val_accuracy: 0.1000\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 100s 6s/step - loss: 2.2998 - accuracy: 0.0988 - val_loss: 2.2917 - val_accuracy: 0.1000\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 104s 6s/step - loss: 2.2939 - accuracy: 0.1389 - val_loss: 2.2717 - val_accuracy: 0.2000\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 106s 6s/step - loss: 2.2321 - accuracy: 0.2500 - val_loss: 2.2125 - val_accuracy: 0.2111\n",
      "Training time: 2276.781 seconds\n",
      "Training time in minutes: 37.946 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-13]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336c1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 700.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 695.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 61s 6s/step - loss: 2.2078 - accuracy: 0.1800\n",
      "\n",
      "val loss: 2.2078\n",
      "val accuracy: 0.1800\n",
      "2/2 [==============================] - 12s 4s/step - loss: 2.1868 - accuracy: 0.1800\n",
      "\n",
      "Testing loss: 2.1868\n",
      "Testing accuracy: 0.1800\n"
     ]
    }
   ],
   "source": [
    "  def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f011d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40822b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15408 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9f623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_17512\\3531255438.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 112s 6s/step - loss: 3.1225 - accuracy: 0.0449 - val_loss: 2.2897 - val_accuracy: 0.1333\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 106s 6s/step - loss: 2.3054 - accuracy: 0.0889 - val_loss: 2.3016 - val_accuracy: 0.1111\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 105s 6s/step - loss: 2.3020 - accuracy: 0.1222 - val_loss: 2.3025 - val_accuracy: 0.1222\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 107s 6s/step - loss: 2.3047 - accuracy: 0.1278 - val_loss: 2.3162 - val_accuracy: 0.0556\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 106s 6s/step - loss: 2.3043 - accuracy: 0.1222 - val_loss: 2.3028 - val_accuracy: 0.0778\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 110s 6s/step - loss: 2.3028 - accuracy: 0.1167 - val_loss: 2.3026 - val_accuracy: 0.1111\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 114s 6s/step - loss: 2.3026 - accuracy: 0.1111 - val_loss: 2.3025 - val_accuracy: 0.0889\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 107s 6s/step - loss: 2.3038 - accuracy: 0.1333 - val_loss: 2.3018 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 104s 6s/step - loss: 2.3036 - accuracy: 0.1222 - val_loss: 2.3025 - val_accuracy: 0.1111\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 111s 6s/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.0889\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 105s 6s/step - loss: 2.3027 - accuracy: 0.0556 - val_loss: 2.3026 - val_accuracy: 0.1333\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 104s 6s/step - loss: 2.3027 - accuracy: 0.0833 - val_loss: 2.3026 - val_accuracy: 0.0889\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 104s 6s/step - loss: 2.3027 - accuracy: 0.0889 - val_loss: 2.3027 - val_accuracy: 0.1111\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 106s 6s/step - loss: 2.3024 - accuracy: 0.1333 - val_loss: 2.3024 - val_accuracy: 0.1222\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 106s 6s/step - loss: 2.3025 - accuracy: 0.1389 - val_loss: 2.3026 - val_accuracy: 0.0778\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 108s 6s/step - loss: 2.3028 - accuracy: 0.0944 - val_loss: 2.3026 - val_accuracy: 0.1222\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 104s 6s/step - loss: 2.3025 - accuracy: 0.1000 - val_loss: 2.3028 - val_accuracy: 0.0444\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 105s 6s/step - loss: 2.3026 - accuracy: 0.1056 - val_loss: 2.3025 - val_accuracy: 0.0889\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 103s 6s/step - loss: 2.3023 - accuracy: 0.1667 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 102s 6s/step - loss: 2.3026 - accuracy: 0.0889 - val_loss: 2.3025 - val_accuracy: 0.1444\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-13]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_1')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d779f9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 640.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 643.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 59s 6s/step - loss: 2.3132 - accuracy: 0.0733\n",
      "\n",
      "val loss: 2.3132\n",
      "val accuracy: 0.0733\n",
      "2/2 [==============================] - 10s 3s/step - loss: 2.3116 - accuracy: 0.0800\n",
      "\n",
      "Testing loss: 2.3116\n",
      "Testing accuracy: 0.0800\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035f114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
