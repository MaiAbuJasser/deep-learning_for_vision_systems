{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1cf4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c5c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a83bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab3400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76dc22e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,454,528\n",
      "Non-trainable params: 260,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-13]:\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02645c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,719,818\n",
      "Trainable params: 14,459,658\n",
      "Non-trainable params: 260,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use “get_layer” method to save the last layer of the network\n",
    "last_layer = base_model.get_layer('global_average_pooling2d') # the name is diffrent not like -5 and -13\n",
    "\n",
    "# save the output of the last layer to be the input of the next layer\n",
    "last_output = last_layer.output\n",
    "\n",
    "# add our new softmax layer with 3 hidden units\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "# instantiate a new_model using keras’s Model class\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# print the new_model summary\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998df16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8641f554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\1835853766.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 99s 6s/step - loss: 3.2552 - accuracy: 0.1000 - val_loss: 2.2986 - val_accuracy: 0.1222\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 117s 7s/step - loss: 2.2893 - accuracy: 0.1000 - val_loss: 2.3104 - val_accuracy: 0.0667\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 112s 6s/step - loss: 2.3000 - accuracy: 0.0833 - val_loss: 2.3014 - val_accuracy: 0.0889\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 109s 6s/step - loss: 2.3003 - accuracy: 0.0611 - val_loss: 2.3023 - val_accuracy: 0.0444\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 114s 6s/step - loss: 2.2950 - accuracy: 0.0667 - val_loss: 2.2873 - val_accuracy: 0.0889\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 99s 6s/step - loss: 2.2897 - accuracy: 0.0988 - val_loss: 2.2824 - val_accuracy: 0.2444\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 98s 5s/step - loss: 2.2577 - accuracy: 0.1163 - val_loss: 2.3325 - val_accuracy: 0.0444\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 100s 6s/step - loss: 2.2607 - accuracy: 0.1222 - val_loss: 2.1849 - val_accuracy: 0.1556\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 98s 6s/step - loss: 2.2769 - accuracy: 0.1444 - val_loss: 2.2142 - val_accuracy: 0.1778\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 99s 6s/step - loss: 2.1353 - accuracy: 0.2278 - val_loss: 2.3422 - val_accuracy: 0.1889\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 102s 6s/step - loss: 2.1547 - accuracy: 0.2167 - val_loss: 2.0380 - val_accuracy: 0.2111\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.9463 - accuracy: 0.2778 - val_loss: 1.9387 - val_accuracy: 0.2667\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 110s 6s/step - loss: 2.1718 - accuracy: 0.2778 - val_loss: 2.1564 - val_accuracy: 0.2222\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 109s 6s/step - loss: 2.1386 - accuracy: 0.1722 - val_loss: 2.0522 - val_accuracy: 0.2222\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 97s 5s/step - loss: 1.9902 - accuracy: 0.2444 - val_loss: 1.8702 - val_accuracy: 0.3556\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 97s 5s/step - loss: 1.7233 - accuracy: 0.3944 - val_loss: 2.0866 - val_accuracy: 0.2222\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.5774 - accuracy: 0.4167 - val_loss: 1.7611 - val_accuracy: 0.3556\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.4649 - accuracy: 0.4556 - val_loss: 1.5727 - val_accuracy: 0.4556\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 98s 6s/step - loss: 1.5437 - accuracy: 0.4778 - val_loss: 1.2773 - val_accuracy: 0.5556\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.3093 - accuracy: 0.5389 - val_loss: 1.1418 - val_accuracy: 0.5556\n",
      "Training time: 2115.920 seconds\n",
      "Training time in minutes: 35.265 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd0decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c13a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "305d280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 694.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3727cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights('signlanguage.model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bfed7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 9s 3s/step - loss: 1.1876 - accuracy: 0.5800\n",
      "\n",
      "Testing loss: 1.1876\n",
      "Testing accuracy: 0.5800\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d9f4ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e93c397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f813dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "182c39c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02c5ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6e2e81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,159,360\n",
      "Non-trainable params: 555,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in base_model.layers[:-12]:\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b40dfac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,719,818\n",
      "Trainable params: 14,164,490\n",
      "Non-trainable params: 555,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "last_layer = base_model.get_layer('global_average_pooling2d_1')\n",
    "\n",
    "# save the output of the last layer to be the input of the next layer\n",
    "last_output = last_layer.output\n",
    "\n",
    "# add our new softmax layer with 3 hidden units\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "# instantiate a new_model using keras’s Model class\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# print the new_model summary\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4b0d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b087984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\1835853766.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 92s 5s/step - loss: 3.6992 - accuracy: 0.1000 - val_loss: 2.3174 - val_accuracy: 0.1444\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 95s 5s/step - loss: 2.2236 - accuracy: 0.1556 - val_loss: 2.1988 - val_accuracy: 0.1556\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 91s 5s/step - loss: 2.2181 - accuracy: 0.1722 - val_loss: 2.1082 - val_accuracy: 0.1556\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 96s 5s/step - loss: 1.9780 - accuracy: 0.1778 - val_loss: 1.8857 - val_accuracy: 0.2444\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.9341 - accuracy: 0.2278 - val_loss: 1.8988 - val_accuracy: 0.2667\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 91s 5s/step - loss: 1.9865 - accuracy: 0.2389 - val_loss: 1.8087 - val_accuracy: 0.3222\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 92s 5s/step - loss: 1.8454 - accuracy: 0.2667 - val_loss: 1.7013 - val_accuracy: 0.3333\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 92s 5s/step - loss: 1.7300 - accuracy: 0.3667 - val_loss: 1.8445 - val_accuracy: 0.2889\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 94s 5s/step - loss: 1.5261 - accuracy: 0.5222 - val_loss: 1.6293 - val_accuracy: 0.4333\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 91s 5s/step - loss: 1.6141 - accuracy: 0.4000 - val_loss: 1.5403 - val_accuracy: 0.4000\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 92s 5s/step - loss: 1.4795 - accuracy: 0.4944 - val_loss: 1.3037 - val_accuracy: 0.5778\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 87s 5s/step - loss: 1.1585 - accuracy: 0.6167 - val_loss: 1.4429 - val_accuracy: 0.4444\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 90s 5s/step - loss: 1.2813 - accuracy: 0.6111 - val_loss: 1.3231 - val_accuracy: 0.5667\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 90s 5s/step - loss: 1.0976 - accuracy: 0.6389 - val_loss: 1.0654 - val_accuracy: 0.6444\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 93s 5s/step - loss: 0.9034 - accuracy: 0.6889 - val_loss: 0.7450 - val_accuracy: 0.7333\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 87s 5s/step - loss: 0.6870 - accuracy: 0.7442 - val_loss: 0.6580 - val_accuracy: 0.7444\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 88s 5s/step - loss: 0.4072 - accuracy: 0.8547 - val_loss: 0.5653 - val_accuracy: 0.7778\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 98s 5s/step - loss: 0.5950 - accuracy: 0.7667 - val_loss: 0.6332 - val_accuracy: 0.8444\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 95s 5s/step - loss: 0.3495 - accuracy: 0.9000 - val_loss: 0.2047 - val_accuracy: 0.9222\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 93s 5s/step - loss: 0.4614 - accuracy: 0.8500 - val_loss: 0.4640 - val_accuracy: 0.8889\n",
      "Training time: 1894.104 seconds\n",
      "Training time in minutes: 31.568 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9c57fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d37c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76201992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 610.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca2692f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights('signlanguage.model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4893cfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 8s 3s/step - loss: 0.8580 - accuracy: 0.6800\n",
      "\n",
      "Testing loss: 0.8580\n",
      "Testing accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "135a1c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "148a8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25e0b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e254c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82cd560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07866554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 13,569,280\n",
      "Non-trainable params: 1,145,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-11]:\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f1c8f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_2   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,719,818\n",
      "Trainable params: 13,574,410\n",
      "Non-trainable params: 1,145,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use “get_layer” method to save the last layer of the network\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_2')\n",
    "\n",
    "# save the output of the last layer to be the input of the next layer\n",
    "last_output = last_layer.output\n",
    "\n",
    "# add our new softmax layer with 3 hidden units\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "# instantiate a new_model using keras’s Model class\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# print the new_model summary\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db037f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8274aca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\1835853766.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 89s 5s/step - loss: 2.7410 - accuracy: 0.2833 - val_loss: 1.6901 - val_accuracy: 0.3667\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 82s 5s/step - loss: 1.4774 - accuracy: 0.4767 - val_loss: 0.9749 - val_accuracy: 0.6444\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 87s 5s/step - loss: 1.2264 - accuracy: 0.5778 - val_loss: 0.9064 - val_accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 91s 5s/step - loss: 0.6977 - accuracy: 0.7667 - val_loss: 0.7112 - val_accuracy: 0.7667\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 88s 5s/step - loss: 0.7474 - accuracy: 0.8000 - val_loss: 0.7499 - val_accuracy: 0.7889\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.4769 - accuracy: 0.8333 - val_loss: 0.2842 - val_accuracy: 0.9222\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 84s 5s/step - loss: 0.2662 - accuracy: 0.9167 - val_loss: 0.4618 - val_accuracy: 0.8778\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 82s 5s/step - loss: 0.2616 - accuracy: 0.9012 - val_loss: 0.3542 - val_accuracy: 0.8889\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 81s 5s/step - loss: 0.1995 - accuracy: 0.9360 - val_loss: 0.2635 - val_accuracy: 0.9556\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 92s 5s/step - loss: 0.2654 - accuracy: 0.9222 - val_loss: 0.2714 - val_accuracy: 0.9111\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 99s 6s/step - loss: 0.0711 - accuracy: 0.9889 - val_loss: 0.1702 - val_accuracy: 0.9556\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 90s 5s/step - loss: 0.0318 - accuracy: 0.9889 - val_loss: 0.0257 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.4279 - accuracy: 0.8833 - val_loss: 0.1090 - val_accuracy: 0.9778\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 84s 5s/step - loss: 0.1534 - accuracy: 0.9444 - val_loss: 0.1054 - val_accuracy: 0.9556\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.0561 - accuracy: 0.9889 - val_loss: 0.0641 - val_accuracy: 0.9889\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 84s 5s/step - loss: 0.0888 - accuracy: 0.9667 - val_loss: 0.4656 - val_accuracy: 0.7889\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 83s 5s/step - loss: 0.1067 - accuracy: 0.9722 - val_loss: 0.0759 - val_accuracy: 0.9556\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 84s 5s/step - loss: 0.0672 - accuracy: 0.9778 - val_loss: 0.2538 - val_accuracy: 0.9778\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.1214 - accuracy: 0.9778 - val_loss: 0.2574 - val_accuracy: 0.9778\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 83s 5s/step - loss: 0.0761 - accuracy: 0.9778 - val_loss: 0.5182 - val_accuracy: 0.9000\n",
      "Training time: 1725.113 seconds\n",
      "Training time in minutes: 28.752 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4920aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9649efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "636bc73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 592.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19d7e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights('signlanguage.model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e74167a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 8s 3s/step - loss: 0.2234 - accuracy: 0.9600\n",
      "\n",
      "Testing loss: 0.2234\n",
      "Testing accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c441eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3fdb8112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a43139ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\2203314509.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 81s 4s/step - loss: 3.6055 - accuracy: 0.0778 - val_loss: 2.2839 - val_accuracy: 0.1667\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 83s 5s/step - loss: 2.3062 - accuracy: 0.1222 - val_loss: 2.2687 - val_accuracy: 0.1778\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 81s 5s/step - loss: 2.2465 - accuracy: 0.1722 - val_loss: 2.1629 - val_accuracy: 0.1889\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 79s 4s/step - loss: 2.2118 - accuracy: 0.1500 - val_loss: 2.0548 - val_accuracy: 0.2444\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 78s 4s/step - loss: 1.9927 - accuracy: 0.2167 - val_loss: 1.9643 - val_accuracy: 0.2778\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 83s 5s/step - loss: 1.8501 - accuracy: 0.2889 - val_loss: 1.8786 - val_accuracy: 0.3000\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 79s 4s/step - loss: 1.6592 - accuracy: 0.3667 - val_loss: 1.5591 - val_accuracy: 0.3667\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 80s 5s/step - loss: 1.5637 - accuracy: 0.4111 - val_loss: 1.4001 - val_accuracy: 0.5778\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 80s 4s/step - loss: 1.4616 - accuracy: 0.4333 - val_loss: 1.2039 - val_accuracy: 0.5778\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 81s 5s/step - loss: 0.9697 - accuracy: 0.6889 - val_loss: 1.1838 - val_accuracy: 0.6111\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 82s 5s/step - loss: 0.8532 - accuracy: 0.6889 - val_loss: 0.9504 - val_accuracy: 0.6444\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 80s 4s/step - loss: 0.8154 - accuracy: 0.6722 - val_loss: 0.7272 - val_accuracy: 0.6778\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 81s 5s/step - loss: 0.5602 - accuracy: 0.8444 - val_loss: 0.7071 - val_accuracy: 0.8667\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 79s 4s/step - loss: 0.4173 - accuracy: 0.8611 - val_loss: 0.3145 - val_accuracy: 0.9333\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 76s 4s/step - loss: 0.2649 - accuracy: 0.9302 - val_loss: 0.3704 - val_accuracy: 0.9333\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 82s 5s/step - loss: 0.1233 - accuracy: 0.9444 - val_loss: 0.4227 - val_accuracy: 0.9333\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 77s 4s/step - loss: 0.2372 - accuracy: 0.9278 - val_loss: 0.4533 - val_accuracy: 0.9111\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 76s 4s/step - loss: 0.1345 - accuracy: 0.9500 - val_loss: 0.6197 - val_accuracy: 0.8778\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 77s 4s/step - loss: 0.1068 - accuracy: 0.9722 - val_loss: 0.2083 - val_accuracy: 0.9667\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 79s 4s/step - loss: 0.1006 - accuracy: 0.9667 - val_loss: 0.1577 - val_accuracy: 0.9444\n",
      "Training time: 1596.257 seconds\n",
      "Training time in minutes: 26.604 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_3')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eff35016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "50751dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 551.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 595.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 53s 5s/step - loss: 0.4518 - accuracy: 0.8300\n",
      "\u000b",
      "al loss: 0.4518\u000b",
      "al accuracy: 0.8300\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.5621 - accuracy: 0.8200\n",
      "\n",
      "Testing loss: 0.5621\n",
      "Testing accuracy: 0.8200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbce2ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86931fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_4   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e284dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\730132684.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 80s 4s/step - loss: 3.1349 - accuracy: 0.1667 - val_loss: 2.1420 - val_accuracy: 0.2222\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 81s 5s/step - loss: 1.9601 - accuracy: 0.3000 - val_loss: 1.5818 - val_accuracy: 0.3778\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 81s 5s/step - loss: 1.4105 - accuracy: 0.4278 - val_loss: 1.3319 - val_accuracy: 0.4889\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 76s 4s/step - loss: 1.1179 - accuracy: 0.5667 - val_loss: 0.8537 - val_accuracy: 0.7000\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 81s 5s/step - loss: 0.7691 - accuracy: 0.7111 - val_loss: 0.7239 - val_accuracy: 0.7778\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 80s 4s/step - loss: 0.6281 - accuracy: 0.7278 - val_loss: 0.6982 - val_accuracy: 0.7667\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 81s 5s/step - loss: 0.5116 - accuracy: 0.8278 - val_loss: 0.4287 - val_accuracy: 0.8778\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 79s 4s/step - loss: 0.5316 - accuracy: 0.8722 - val_loss: 0.7912 - val_accuracy: 0.7333\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 75s 4s/step - loss: 0.5864 - accuracy: 0.8023 - val_loss: 0.2830 - val_accuracy: 0.9444\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 74s 4s/step - loss: 0.3036 - accuracy: 0.9128 - val_loss: 0.2650 - val_accuracy: 0.9111\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 75s 4s/step - loss: 0.5638 - accuracy: 0.8372 - val_loss: 0.2512 - val_accuracy: 0.9000\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 75s 4s/step - loss: 0.1237 - accuracy: 0.9709 - val_loss: 0.1454 - val_accuracy: 0.9444\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 77s 4s/step - loss: 0.1535 - accuracy: 0.9611 - val_loss: 0.2161 - val_accuracy: 0.9444\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 77s 4s/step - loss: 0.3192 - accuracy: 0.9111 - val_loss: 0.1993 - val_accuracy: 0.9333\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 79s 4s/step - loss: 0.1229 - accuracy: 0.9556 - val_loss: 0.0747 - val_accuracy: 0.9778\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 76s 4s/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.1190 - val_accuracy: 0.9667\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 73s 4s/step - loss: 0.0647 - accuracy: 0.9709 - val_loss: 0.0258 - val_accuracy: 0.9889\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 75s 4s/step - loss: 0.0346 - accuracy: 0.9833 - val_loss: 0.1348 - val_accuracy: 0.9333\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 77s 4s/step - loss: 0.0759 - accuracy: 0.9778 - val_loss: 0.2695 - val_accuracy: 0.9667\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 77s 4s/step - loss: 0.0288 - accuracy: 0.9944 - val_loss: 0.3197 - val_accuracy: 0.9778\n",
      "Training time: 1550.216 seconds\n",
      "Training time in minutes: 25.837 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_4')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b76cc128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 616.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 747.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 52s 5s/step - loss: 0.1488 - accuracy: 0.9733\n",
      "\n",
      "val loss: 0.1488\n",
      "val accuracy: 0.9733\n",
      "2/2 [==============================] - 9s 3s/step - loss: 0.1441 - accuracy: 0.9400\n",
      "\n",
      "Testing loss: 0.1441\n",
      "Testing accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7965b3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed60f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3fd6315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_5   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24e2ecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\715346089.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 73s 4s/step - loss: 2.8848 - accuracy: 0.1667 - val_loss: 1.9062 - val_accuracy: 0.2444\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 1.9344 - accuracy: 0.3000 - val_loss: 1.5621 - val_accuracy: 0.4333\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 67s 4s/step - loss: 1.3726 - accuracy: 0.5058 - val_loss: 1.2883 - val_accuracy: 0.5333\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 1.2115 - accuracy: 0.5333 - val_loss: 0.9527 - val_accuracy: 0.6778\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 1.0401 - accuracy: 0.6000 - val_loss: 0.8833 - val_accuracy: 0.7111\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.6921 - accuracy: 0.7778 - val_loss: 0.6271 - val_accuracy: 0.7889\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 0.4116 - accuracy: 0.8372 - val_loss: 0.4791 - val_accuracy: 0.8444\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 67s 4s/step - loss: 0.3011 - accuracy: 0.9012 - val_loss: 0.4818 - val_accuracy: 0.8667\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.3675 - accuracy: 0.8833 - val_loss: 1.5193 - val_accuracy: 0.4778\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.5285 - accuracy: 0.8389 - val_loss: 0.4487 - val_accuracy: 0.7889\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 72s 4s/step - loss: 0.3078 - accuracy: 0.8889 - val_loss: 0.1987 - val_accuracy: 0.9333\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.1345 - accuracy: 0.9611 - val_loss: 0.3534 - val_accuracy: 0.9000\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.1499 - accuracy: 0.9444 - val_loss: 0.3261 - val_accuracy: 0.9222\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.0617 - accuracy: 0.9833 - val_loss: 0.1655 - val_accuracy: 0.9444\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 70s 4s/step - loss: 0.0346 - accuracy: 0.9944 - val_loss: 0.1367 - val_accuracy: 0.9667\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.2193 - accuracy: 0.9186 - val_loss: 0.5810 - val_accuracy: 0.8778\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.4904 - accuracy: 0.8611 - val_loss: 0.2333 - val_accuracy: 0.9333\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.1909 - accuracy: 0.9333 - val_loss: 0.6771 - val_accuracy: 0.8556\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 69s 4s/step - loss: 0.2644 - accuracy: 0.9500 - val_loss: 0.2292 - val_accuracy: 0.9556\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 71s 4s/step - loss: 0.0953 - accuracy: 0.9722 - val_loss: 0.1306 - val_accuracy: 0.9444\n",
      "Training time: 1393.190 seconds\n",
      "Training time in minutes: 23.220 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-8]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_5')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ecb7408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 345.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 718.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 52s 5s/step - loss: 0.2722 - accuracy: 0.9267\n",
      "\n",
      "val loss: 0.2722\n",
      "val accuracy: 0.9267\n",
      "2/2 [==============================] - 9s 3s/step - loss: 0.1893 - accuracy: 0.9400\n",
      "\n",
      "Testing loss: 0.1893\n",
      "Testing accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b45b2dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abe111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2209ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_6   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85562020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\1245896272.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 65s 4s/step - loss: 3.9831 - accuracy: 0.1278 - val_loss: 2.2380 - val_accuracy: 0.0778\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 66s 4s/step - loss: 2.2321 - accuracy: 0.2000 - val_loss: 2.0005 - val_accuracy: 0.3556\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 68s 4s/step - loss: 2.0043 - accuracy: 0.2722 - val_loss: 1.7777 - val_accuracy: 0.3333\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 66s 4s/step - loss: 1.6832 - accuracy: 0.3556 - val_loss: 1.2993 - val_accuracy: 0.5333\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 65s 4s/step - loss: 1.2823 - accuracy: 0.5500 - val_loss: 1.1083 - val_accuracy: 0.5444\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 64s 4s/step - loss: 1.1375 - accuracy: 0.5407 - val_loss: 0.9089 - val_accuracy: 0.7000\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 63s 4s/step - loss: 0.7924 - accuracy: 0.6944 - val_loss: 0.8806 - val_accuracy: 0.6444\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 65s 4s/step - loss: 0.8280 - accuracy: 0.7035 - val_loss: 0.6790 - val_accuracy: 0.7778\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 63s 4s/step - loss: 0.7198 - accuracy: 0.7500 - val_loss: 0.5269 - val_accuracy: 0.7778\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 65s 4s/step - loss: 0.5027 - accuracy: 0.8556 - val_loss: 0.3456 - val_accuracy: 0.8667\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 62s 3s/step - loss: 0.1999 - accuracy: 0.9500 - val_loss: 0.2595 - val_accuracy: 0.9111\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 62s 3s/step - loss: 0.2407 - accuracy: 0.9333 - val_loss: 0.2699 - val_accuracy: 0.9000\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 63s 4s/step - loss: 0.0977 - accuracy: 0.9833 - val_loss: 0.1229 - val_accuracy: 0.9778\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 61s 3s/step - loss: 0.1423 - accuracy: 0.9500 - val_loss: 0.1192 - val_accuracy: 0.9556\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 62s 3s/step - loss: 0.1600 - accuracy: 0.9500 - val_loss: 0.2399 - val_accuracy: 0.9556\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 62s 3s/step - loss: 0.0844 - accuracy: 0.9778 - val_loss: 0.0820 - val_accuracy: 0.9667\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 62s 3s/step - loss: 0.0724 - accuracy: 0.9889 - val_loss: 0.2120 - val_accuracy: 0.9889\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 64s 4s/step - loss: 0.3037 - accuracy: 0.9278 - val_loss: 1.3565 - val_accuracy: 0.6556\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 61s 3s/step - loss: 0.4621 - accuracy: 0.8889 - val_loss: 0.3433 - val_accuracy: 0.9000\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 62s 4s/step - loss: 0.1047 - accuracy: 0.9667 - val_loss: 0.3340 - val_accuracy: 0.9111\n",
      "Training time: 1288.930 seconds\n",
      "Training time in minutes: 21.482 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_6')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de150340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 753.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 629.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 55s 5s/step - loss: 0.5693 - accuracy: 0.7800\n",
      "\n",
      "val loss: 0.5693\n",
      "val accuracy: 0.7800\n",
      "2/2 [==============================] - 9s 3s/step - loss: 0.6280 - accuracy: 0.7400\n",
      "\n",
      "Testing loss: 0.6280\n",
      "Testing accuracy: 0.7400\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a726dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e03f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "573435b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_7   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad3d5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\340087587.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 56s 3s/step - loss: 3.6944 - accuracy: 0.2111 - val_loss: 2.1668 - val_accuracy: 0.2667\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 1.9889 - accuracy: 0.3000 - val_loss: 1.7775 - val_accuracy: 0.3444\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 60s 3s/step - loss: 1.4162 - accuracy: 0.4944 - val_loss: 1.2715 - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.9681 - accuracy: 0.7167 - val_loss: 0.5757 - val_accuracy: 0.7667\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 61s 3s/step - loss: 0.5353 - accuracy: 0.8000 - val_loss: 0.3329 - val_accuracy: 0.9111\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 61s 3s/step - loss: 0.2538 - accuracy: 0.9278 - val_loss: 0.3456 - val_accuracy: 0.9000\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.2873 - accuracy: 0.9070 - val_loss: 0.3366 - val_accuracy: 0.9111\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.2655 - accuracy: 0.9111 - val_loss: 0.3898 - val_accuracy: 0.8556\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 0.1378 - accuracy: 0.9556 - val_loss: 0.5073 - val_accuracy: 0.8222\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 0.1759 - accuracy: 0.9444 - val_loss: 0.1999 - val_accuracy: 0.9444\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.0803 - accuracy: 0.9889 - val_loss: 0.1223 - val_accuracy: 0.9556\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 0.0669 - accuracy: 0.9667 - val_loss: 0.2437 - val_accuracy: 0.9333\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.1233 - accuracy: 0.9611 - val_loss: 0.2284 - val_accuracy: 0.9333\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 55s 3s/step - loss: 0.0924 - accuracy: 0.9833 - val_loss: 0.3053 - val_accuracy: 0.9000\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.0721 - accuracy: 0.9722 - val_loss: 0.0487 - val_accuracy: 0.9889\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 55s 3s/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.1174 - val_accuracy: 0.9444\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.0143 - accuracy: 0.9942 - val_loss: 0.0223 - val_accuracy: 0.9889\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1743 - val_accuracy: 0.9778\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 55s 3s/step - loss: 0.0140 - accuracy: 0.9944 - val_loss: 0.1544 - val_accuracy: 0.9889\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 0.0424 - accuracy: 0.9889 - val_loss: 0.1001 - val_accuracy: 0.9556\n",
      "Training time: 1153.995 seconds\n",
      "Training time in minutes: 19.233 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-6]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_7')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1283204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 757.25it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 617.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 53s 5s/step - loss: 0.2718 - accuracy: 0.9267\n",
      "\n",
      "val loss: 0.2718\n",
      "val accuracy: 0.9267\n",
      "2/2 [==============================] - 9s 3s/step - loss: 0.2331 - accuracy: 0.9600\n",
      "\n",
      "Testing loss: 0.2331\n",
      "Testing accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41fa4a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a7635056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_8   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9cf591f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\3823079013.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 55s 3s/step - loss: 3.5744 - accuracy: 0.1500 - val_loss: 2.0127 - val_accuracy: 0.2556\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 1.7277 - accuracy: 0.3605 - val_loss: 1.6559 - val_accuracy: 0.4000\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 1.2374 - accuracy: 0.5722 - val_loss: 1.1547 - val_accuracy: 0.5667\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.8160 - accuracy: 0.6722 - val_loss: 0.5609 - val_accuracy: 0.8556\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.4655 - accuracy: 0.8278 - val_loss: 0.3344 - val_accuracy: 0.9000\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.2655 - accuracy: 0.9111 - val_loss: 0.2051 - val_accuracy: 0.9222\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 58s 3s/step - loss: 0.2778 - accuracy: 0.9500 - val_loss: 0.1553 - val_accuracy: 0.9444\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.1387 - accuracy: 0.9556 - val_loss: 0.2797 - val_accuracy: 0.9111\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.1499 - accuracy: 0.9389 - val_loss: 0.1822 - val_accuracy: 0.9556\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.2928 - accuracy: 0.9012 - val_loss: 0.2513 - val_accuracy: 0.8889\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 0.1383 - accuracy: 0.9667 - val_loss: 0.1113 - val_accuracy: 0.9667\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 59s 3s/step - loss: 0.1114 - accuracy: 0.9667 - val_loss: 0.1853 - val_accuracy: 0.9444\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.1307 - accuracy: 0.9667 - val_loss: 0.1557 - val_accuracy: 0.9778\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 0.0613 - accuracy: 0.9833 - val_loss: 0.1774 - val_accuracy: 0.9667\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.0509 - accuracy: 0.9833 - val_loss: 0.1991 - val_accuracy: 0.9778\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 55s 3s/step - loss: 0.0394 - accuracy: 0.9884 - val_loss: 0.1586 - val_accuracy: 0.9889\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.1734 - val_accuracy: 0.9778\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 59s 3s/step - loss: 0.0240 - accuracy: 0.9944 - val_loss: 0.2384 - val_accuracy: 0.9556\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 60s 3s/step - loss: 0.0614 - accuracy: 0.9833 - val_loss: 0.2534 - val_accuracy: 0.9556\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 55s 3s/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2724 - val_accuracy: 0.9222\n",
      "Training time: 1148.776 seconds\n",
      "Training time in minutes: 19.146 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_8')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77422c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 607.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 628.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 56s 6s/step - loss: 0.2651 - accuracy: 0.9300\n",
      "\n",
      "val loss: 0.2651\n",
      "val accuracy: 0.9300\n",
      "2/2 [==============================] - 9s 3s/step - loss: 0.1679 - accuracy: 0.9600\n",
      "\n",
      "Testing loss: 0.1679\n",
      "Testing accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb290ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d2c878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_9   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0fe9358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\1202332932.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 76s 4s/step - loss: 3.0929 - accuracy: 0.3556 - val_loss: 1.2493 - val_accuracy: 0.5889\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 57s 3s/step - loss: 0.8648 - accuracy: 0.7000 - val_loss: 0.6802 - val_accuracy: 0.7222\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.4845 - accuracy: 0.8278 - val_loss: 0.3364 - val_accuracy: 0.8778\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.2358 - accuracy: 0.9222 - val_loss: 0.2087 - val_accuracy: 0.9444\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.2649 - accuracy: 0.9056 - val_loss: 0.2561 - val_accuracy: 0.9000\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 55s 3s/step - loss: 0.1290 - accuracy: 0.9667 - val_loss: 0.1549 - val_accuracy: 0.9667\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.0780 - accuracy: 0.9884 - val_loss: 0.1585 - val_accuracy: 0.9333\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.0824 - accuracy: 0.9833 - val_loss: 0.1273 - val_accuracy: 0.9444\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.0839 - accuracy: 0.9611 - val_loss: 0.1118 - val_accuracy: 0.9667\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.0443 - accuracy: 0.9833 - val_loss: 0.3140 - val_accuracy: 0.9333\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.0882 - accuracy: 0.9767 - val_loss: 0.1041 - val_accuracy: 0.9778\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.0620 - accuracy: 0.9667 - val_loss: 0.1133 - val_accuracy: 0.9667\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.0670 - accuracy: 0.9778 - val_loss: 0.2384 - val_accuracy: 0.9556\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.1466 - accuracy: 0.9722 - val_loss: 0.1190 - val_accuracy: 0.9556\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.0216 - accuracy: 0.9944 - val_loss: 0.2237 - val_accuracy: 0.9667\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 55s 3s/step - loss: 0.0391 - accuracy: 0.9944 - val_loss: 0.0288 - val_accuracy: 0.9889\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.0214 - accuracy: 0.9884 - val_loss: 0.0444 - val_accuracy: 0.9889\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.0352 - accuracy: 0.9944 - val_loss: 0.1992 - val_accuracy: 0.9778\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.0303 - accuracy: 0.9833 - val_loss: 0.1312 - val_accuracy: 0.9556\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 58s 3s/step - loss: 0.0143 - accuracy: 0.9889 - val_loss: 0.0676 - val_accuracy: 0.9667\n",
      "Training time: 1110.850 seconds\n",
      "Training time in minutes: 18.514 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_9')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9b28f050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 724.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 748.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 51s 5s/step - loss: 0.4172 - accuracy: 0.8767\n",
      "\n",
      "val loss: 0.4172\n",
      "val accuracy: 0.8767\n",
      "2/2 [==============================] - 9s 3s/step - loss: 0.2522 - accuracy: 0.9200\n",
      "\n",
      "Testing loss: 0.2522\n",
      "Testing accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc707454",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b8dcc3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1712 images belonging to 10 classes.\n",
      "Found 300 images belonging to 10 classes.\n",
      "Found 50 images belonging to 10 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_10  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/train'\n",
    "valid_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid'\n",
    "test_path  = 'C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test'\n",
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=10)\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(224,224), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(224,224), \n",
    "                                                        batch_size=50, \n",
    "                                                        shuffle=False)\n",
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224,224, 3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9de08ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subeh\\AppData\\Local\\Temp\\ipykernel_15096\\2371742377.py:18: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 48s 3s/step - loss: 5.2498 - accuracy: 0.2833 - val_loss: 2.7628 - val_accuracy: 0.4889\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 51s 3s/step - loss: 1.6586 - accuracy: 0.5944 - val_loss: 1.5709 - val_accuracy: 0.6778\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 1.3343 - accuracy: 0.7111 - val_loss: 0.8487 - val_accuracy: 0.7222\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 51s 3s/step - loss: 0.4683 - accuracy: 0.8833 - val_loss: 0.8484 - val_accuracy: 0.7444\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 50s 3s/step - loss: 0.4076 - accuracy: 0.8779 - val_loss: 0.6249 - val_accuracy: 0.8000\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 51s 3s/step - loss: 0.3577 - accuracy: 0.8944 - val_loss: 0.3572 - val_accuracy: 0.9000\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.0976 - accuracy: 0.9778 - val_loss: 0.3038 - val_accuracy: 0.8889\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.2161 - accuracy: 0.9333 - val_loss: 0.3157 - val_accuracy: 0.9222\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 56s 3s/step - loss: 0.1464 - accuracy: 0.9500 - val_loss: 0.3876 - val_accuracy: 0.8778\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.1374 - accuracy: 0.9444 - val_loss: 0.4067 - val_accuracy: 0.9000\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.1556 - accuracy: 0.9500 - val_loss: 0.1615 - val_accuracy: 0.9444\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 53s 3s/step - loss: 0.0949 - accuracy: 0.9667 - val_loss: 0.0709 - val_accuracy: 0.9667\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.0914 - accuracy: 0.9722 - val_loss: 0.1369 - val_accuracy: 0.9444\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.0848 - accuracy: 0.9722 - val_loss: 0.3061 - val_accuracy: 0.9333\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 54s 3s/step - loss: 0.0753 - accuracy: 0.9667 - val_loss: 0.0674 - val_accuracy: 0.9667\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.0966 - accuracy: 0.9722 - val_loss: 0.1485 - val_accuracy: 0.9667\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.0342 - accuracy: 0.9944 - val_loss: 0.2628 - val_accuracy: 0.9000\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.0954 - accuracy: 0.9556 - val_loss: 0.2178 - val_accuracy: 0.9444\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 51s 3s/step - loss: 0.0322 - accuracy: 0.9942 - val_loss: 0.1657 - val_accuracy: 0.9556\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 52s 3s/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.2644 - val_accuracy: 0.9333\n",
      "Training time: 1047.338 seconds\n",
      "Training time in minutes: 17.456 minutes\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_10')\n",
    "\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = Dense(10, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "new_model.compile(Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "import time\n",
    "start_time = time.time()\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='signlanguage.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=20, verbose=1, callbacks=[checkpointer])\n",
    "end_time = time.time()\n",
    "train_time_in_minutes = (end_time - start_time) / 60.0\n",
    "train_time = end_time - start_time\n",
    "print(f\"Training time: {train_time:.3f} seconds\")\n",
    "print(f\"Training time in minutes: {train_time_in_minutes:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "437c2493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 817.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 796.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 51s 5s/step - loss: 0.9238 - accuracy: 0.6967\n",
      "\n",
      "val loss: 0.9238\n",
      "val accuracy: 0.6967\n",
      "2/2 [==============================] - 10s 4s/step - loss: 0.6895 - accuracy: 0.7400\n",
      "\n",
      "Testing loss: 0.6895\n",
      "Testing accuracy: 0.7400\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/test')\n",
    "valid_files, valid_targets = load_dataset('C:/Users/subeh/OneDrive/Desktop/deep_learning_for_vision_systems/chapter_06/sign_language_project/dataset/valid')\n",
    "\n",
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image #wrire this to solve the error 'image.load_img'\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))\n",
    "val_tensors = preprocess_input(paths_to_tensor(valid_files))\n",
    "\n",
    "new_model.load_weights('signlanguage.model.hdf5')\n",
    "print('\\nval loss: {:.4f}\\nval accuracy: {:.4f}'.format(*new_model.evaluate(val_tensors, valid_targets)))\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a79f6bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [3, 4, 5, 6, 7,8,9,10,11,12,13]\n",
    "y = [0.6967, 0.8767 , 0.93 , 0.9267 , 0.78 , 0.9267 , 0.9733 , 0.83 , 0.94  , 0.68 , 0.59]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Plot of x vs y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906d9d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
